{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "from scipy import stats\n",
    "from common import (\n",
    "    load_data,\n",
    "    pareto_rank,\n",
    "    baseline_results,\n",
    "    DecisionTreeClassifierWithMultipleLabels,\n",
    "    DecisionTreeClassifierWithMultipleLabelsPandas\n",
    ")\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "random_state = 1234\n",
    "test_size = 0.40\n",
    "pareto_cutoff = 0.5\n",
    "rank_by_domination_count = True\n",
    "\n",
    "# performances = [\"fps\", \"cpu\"]\n",
    "# performances = [\"kbs\", \"fps\"]\n",
    "performances = []\n",
    "\n",
    "(\n",
    "    perf_matrix_initial,\n",
    "    input_features,\n",
    "    config_features,\n",
    "    all_performances,\n",
    "    input_preprocessor,\n",
    "    config_preprocessor,\n",
    ") = load_data(system=\"x264\", data_dir=\"../data\", input_properties_type=\"tabular\")\n",
    "\n",
    "if len(performances) == 0:\n",
    "    performances = all_performances\n",
    "\n",
    "# Normalization is needed for the Pareto cutoff\n",
    "# We can normalize before splitting, because\n",
    "# we normalize per input and we also split per input.\n",
    "# There is no data leakage.\n",
    "normalized_metrics = (\n",
    "    perf_matrix_initial[[\"inputname\"] + performances]\n",
    "    .groupby(\"inputname\", as_index=False)\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    ")\n",
    "cutoff_mask = (normalized_metrics <= pareto_cutoff).all(axis=1)\n",
    "\n",
    "nmdf = (\n",
    "    perf_matrix_initial[[\"inputname\"] + performances]\n",
    "    .groupby(\"inputname\", as_index=True)\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    ")\n",
    "perf_matrix = pd.merge(\n",
    "    perf_matrix_initial, nmdf, suffixes=(\"_raw\", None), left_index=True, right_index=True\n",
    ")\n",
    "perf_matrix[\"feasible\"] = cutoff_mask\n",
    "\n",
    "all_perf_raw = [f\"{p}_raw\" for p in performances]\n",
    "all_perf_norm = [f\"{p}\" for p in performances]\n",
    "\n",
    "icm_all = (\n",
    "    perf_matrix[[\"inputname\", \"configurationID\"] + performances]\n",
    "    .sort_values([\"inputname\", \"configurationID\"])\n",
    "    .set_index([\"inputname\", \"configurationID\"])\n",
    ")\n",
    "icm_ranked_measures = icm_all.groupby(\n",
    "    \"inputname\"\n",
    ").transform(  # Go from measured values to ranks within each input group\n",
    "    lambda x: stats.rankdata(x, method=\"min\")\n",
    ")\n",
    "icm_all[\"ranks\"] = icm_all.groupby(\"inputname\", group_keys=False).apply(\n",
    "    lambda x: pareto_rank(\n",
    "        x, cutoff=pareto_cutoff, rank_by_domination_count=rank_by_domination_count\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and preprocess further\n",
    "train_inp, test_inp = train_test_split(\n",
    "    perf_matrix[\"inputname\"].unique(),\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    ")\n",
    "train_perf = perf_matrix[perf_matrix.inputname.isin(train_inp)].copy()\n",
    "test_perf = perf_matrix[perf_matrix.inputname.isin(test_inp)]\n",
    "\n",
    "dropped_measurements = train_perf.sample(frac=0.9, random_state=1337)\n",
    "train_perf.drop(dropped_measurements.index, inplace=True)\n",
    "\n",
    "icm = (\n",
    "    train_perf[[\"inputname\", \"configurationID\"] + performances]\n",
    "    .sort_values([\"inputname\", \"configurationID\"])\n",
    "    .set_index([\"inputname\", \"configurationID\"])\n",
    ")\n",
    "icm_ranked_measures = icm.groupby(\n",
    "    \"inputname\"\n",
    ").transform(  # Go from measured values to ranks within each input group\n",
    "    lambda x: stats.rankdata(x, method=\"min\")\n",
    ")\n",
    "icm[\"ranks\"] = icm.groupby(\"inputname\", group_keys=False).apply(\n",
    "    lambda x: pareto_rank(\n",
    "        x, cutoff=pareto_cutoff, rank_by_domination_count=rank_by_domination_count\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate the Pareto ranks for the test data\n",
    "icm_test = (\n",
    "    test_perf[[\"inputname\", \"configurationID\"] + performances]\n",
    "    .sort_values([\"inputname\", \"configurationID\"])\n",
    "    .set_index([\"inputname\", \"configurationID\"])\n",
    ")\n",
    "icm_test[\"ranks\"] = icm_test.groupby(\"inputname\", group_keys=False).apply(\n",
    "    lambda x: pareto_rank(\n",
    "        x, cutoff=pareto_cutoff, rank_by_domination_count=rank_by_domination_count\n",
    "    )\n",
    ")\n",
    "\n",
    "# Full dataset of input features + config features that are in the first rank\n",
    "dataset = icm[icm.ranks <= 1].join(config_features).join(input_features).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rank of the overall best configuration: 2.23+-2.25\n",
      "Average rank of the most common configuration: 2.23+-2.25\n",
      "Average rank of the best configuration for all metrics: 4.92+-1.82\n",
      "Average rank of random configuration: 3.83+-2.40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall': [2.2349514563106796, 2.246731683222051],\n",
       " 'metric': [4.918446601941747, 1.8151243358010498],\n",
       " 'common': [2.2349514563106796, 2.246731683222051],\n",
       " 'random': [3.825631067961165, 2.404171000865784]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline results\n",
    "baseline_results(icm, icm_ranked_measures, icm_test, dataset, config_features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rank 3.258252427184466\n"
     ]
    }
   ],
   "source": [
    "## We make a multi-class classification problem\n",
    "# Each input is annotated with the rank-1 classes\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(dataset[\"configurationID\"].tolist())\n",
    "\n",
    "grouped_df = dataset.groupby(\"inputname\")[\"configurationID\"].apply(enc.transform).reset_index()\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit and transform the 'Values' column\n",
    "binary_matrix = mlb.fit_transform(grouped_df['configurationID'])\n",
    "\n",
    "# Create a new DataFrame with the binary matrix\n",
    "binary_df = pd.DataFrame(binary_matrix, columns=mlb.classes_, index=grouped_df['inputname'])\n",
    "\n",
    "X = input_preprocessor.fit_transform(\n",
    "    input_features[input_features.index.get_level_values(\"inputname\").isin(train_inp)].sort_index()\n",
    ")\n",
    "y = binary_df.values\n",
    "\n",
    "clf = DecisionTreeClassifierWithMultipleLabels(max_depth=X.shape[1])\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)\n",
    "\n",
    "X_test = input_preprocessor.transform(\n",
    "    input_features.query(\"inputname.isin(@test_inp)\").sort_index()\n",
    ")\n",
    "pred_cfg_test = enc.inverse_transform(clf.predict(X_test)).astype(int)\n",
    "\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(test_inp, pred_cfg_test), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "print(\n",
    "    \"Test rank\",\n",
    "    icm_test.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\"ranks\"].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rank 3.153398058252427\n"
     ]
    }
   ],
   "source": [
    "Xpd = input_features[input_features.index.get_level_values(\"inputname\").isin(train_inp)].sort_index()\n",
    "Xpd_test = input_features[input_features.index.get_level_values(\"inputname\").isin(test_inp)].sort_index()\n",
    "\n",
    "clf = DecisionTreeClassifierWithMultipleLabelsPandas(max_depth=Xpd.shape[1])\n",
    "clf.fit(Xpd, y)\n",
    "clf.score(Xpd, y)\n",
    "\n",
    "pred_cfg_test = enc.inverse_transform(clf.predict(Xpd_test)).astype(int)\n",
    "\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(test_inp, pred_cfg_test), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "print(\n",
    "    \"Test rank\",\n",
    "    icm_test.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\"ranks\"].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.unique_leaf_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m X_test \u001b[38;5;241m=\u001b[39m input_preprocessor\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m      2\u001b[0m     input_features\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputname.isin(@test_inp)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m pred_cfg_test \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      6\u001b[0m inp_pred_map \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mzip\u001b[39m(test_inp, pred_cfg_test), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurationID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest rank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     icm_test\u001b[38;5;241m.\u001b[39mmerge(inp_pred_map, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurationID\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Sandbox/resist/partial-configuration/src/common.py:690\u001b[0m, in \u001b[0;36mDecisionTree.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree) \u001b[38;5;28;01mfor\u001b[39;00m _, x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m()])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "X_test = input_preprocessor.transform(\n",
    "    input_features.query(\"inputname.isin(@test_inp)\").sort_index()\n",
    ")\n",
    "pred_cfg_test = enc.inverse_transform(clf.predict(X_test)).astype(int)\n",
    "\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(test_inp, pred_cfg_test), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "print(\n",
    "    \"Test rank\",\n",
    "    icm_test.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\"ranks\"].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resolution                      int64\n",
       "WIDTH                           int64\n",
       "HEIGHT                          int64\n",
       "SPATIAL_COMPLEXITY            float64\n",
       "TEMPORAL_COMPLEXITY           float64\n",
       "CHUNK_COMPLEXITY_VARIATION    float64\n",
       "COLOR_COMPLEXITY              float64\n",
       "category                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[\n",
    "        input_features.index.get_level_values(\"inputname\").isin(train_inp)\n",
    "    ].sort_index().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=1, Train score=0.52, Val score=0.42, Val rank=3.44\n",
      "Depth=2, Train score=0.54, Val score=0.52, Val rank=3.77\n",
      "Depth=3, Train score=0.55, Val score=0.46, Val rank=3.64\n",
      "Depth=4, Train score=0.61, Val score=0.48, Val rank=3.90\n",
      "Depth=5, Train score=0.66, Val score=0.52, Val rank=3.74\n",
      "Depth=6, Train score=0.74, Val score=0.50, Val rank=3.90\n",
      "Depth=7, Train score=0.86, Val score=0.45, Val rank=3.71\n",
      "Depth=8, Train score=0.96, Val score=0.42, Val rank=3.76\n",
      "Depth=9, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=10, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=11, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=12, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=13, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=14, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=15, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=16, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=17, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=18, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=19, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=20, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Depth=21, Train score=1.00, Val score=0.41, Val rank=3.78\n",
      "Best depth 5\n",
      "Scores 0.6398963730569949\n",
      "Train rank 4.2625607779578605\n",
      "Val rank 3.9612903225806453\n",
      "Test rank 4.466019417475728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(dataset[\"configurationID\"].tolist())\n",
    "\n",
    "grouped_df = (\n",
    "    dataset.groupby(\"inputname\")[\"configurationID\"].apply(enc.transform).reset_index()\n",
    ")\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit and transform the 'Values' column\n",
    "binary_matrix = mlb.fit_transform(grouped_df[\"configurationID\"])\n",
    "\n",
    "# Create a new DataFrame with the binary matrix\n",
    "binary_df = pd.DataFrame(\n",
    "    binary_matrix, columns=mlb.classes_, index=grouped_df[\"inputname\"]\n",
    ")\n",
    "\n",
    "X = input_preprocessor.fit_transform(\n",
    "    input_features[\n",
    "        input_features.index.get_level_values(\"inputname\").isin(train_inp)\n",
    "    ].sort_index()\n",
    ")\n",
    "y = binary_df.values\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(X.shape[0]), test_size=0.2, random_state=random_state\n",
    ")\n",
    "X_train = X[train_idx]\n",
    "X_val = X[val_idx]\n",
    "y_train = y[train_idx]\n",
    "y_val = y[val_idx]\n",
    "inputnames_train = train_inp[train_idx]\n",
    "inputnames_val = train_inp[val_idx]\n",
    "\n",
    "best_val_rank = 100_000\n",
    "best_depth = 0\n",
    "\n",
    "for i in range(1, X.shape[1] + 1):\n",
    "    clf = DecisionTreeClassifierWithMultipleLabels(max_depth=i)\n",
    "    # clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_score = clf.score(X_val, y_val)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "\n",
    "    # Validation test\n",
    "    pred_cfg_lbl = clf.predict(X_val)\n",
    "    pred_cfg = enc.inverse_transform(pred_cfg_lbl).astype(int)\n",
    "    inp_pred_map = pd.DataFrame(\n",
    "        zip(inputnames_val, pred_cfg), columns=[\"inputname\", \"configurationID\"]\n",
    "    )\n",
    "    val_rank = icm.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\n",
    "        \"ranks\"\n",
    "    ].mean()\n",
    "\n",
    "    print(\n",
    "        f\"Depth={i}, Train score={train_score:.2f}, Val score={val_score:.2f}, Val rank={val_rank:.2f}\"\n",
    "    )\n",
    "\n",
    "    if val_rank < best_val_rank:\n",
    "        best_val_rank = val_rank\n",
    "        best_depth = i\n",
    "\n",
    "print(f\"Best depth {best_depth}\")\n",
    "clf = DecisionTreeClassifierWithMultipleLabels(max_depth=best_depth)\n",
    "\n",
    "# Test on whole training set\n",
    "clf.fit(X, y)\n",
    "pred_cfg = enc.inverse_transform(clf.predict(X)).astype(int)\n",
    "\n",
    "print(\"Scores\", clf.score(X, y))\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(inputnames_train, pred_cfg[train_idx]), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "train_rank = icm.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\n",
    "    \"ranks\"\n",
    "].mean()\n",
    "print(\"Train rank\", train_rank)\n",
    "\n",
    "# Validation test\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(inputnames_val, pred_cfg[val_idx]), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "val_rank = icm.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\"ranks\"].mean()\n",
    "print(\"Val rank\", val_rank)\n",
    "\n",
    "# Test set\n",
    "X_test = input_preprocessor.transform(input_features.query(\"inputname.isin(@test_inp)\"))\n",
    "pred_cfg_test = enc.inverse_transform(clf.predict(X_test)).astype(int)\n",
    "\n",
    "inp_pred_map = pd.DataFrame(\n",
    "    zip(test_inp, pred_cfg_test), columns=[\"inputname\", \"configurationID\"]\n",
    ")\n",
    "print(\n",
    "    \"Test rank\",\n",
    "    icm_test.merge(inp_pred_map, on=[\"inputname\", \"configurationID\"])[\"ranks\"].mean(),\n",
    ")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
